{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating \"Gold\" data\n",
    "\n",
    "Use the `large-v3` model as baseline:\n",
    "\n",
    "- Perform prediction on all audio files and use that as \"ground truth\"\n",
    "- Why use these as labels?\n",
    "\t- The labels provided by NUHS are not perfect, and audio mixing are often intertwined between multiple speakers (i.e., patient, SLT, family member, etc.)\n",
    "    - Some labels are more \"complete\" than others\n",
    "\t- Consistency issues within labels: Some human labellers may ignore noise of a certain threshold - Utilising Whisper as \"Ground Truth\" helps with that inconsistency\n",
    "\t- The end goal: To improve the performance of Whisper for Aphasic Patients in a localised context (Singapore)\n",
    "- Use the new ground truth transcriptions + audio and train the `small.en` model\n",
    "\t- This fine-tuned model should perform better than the \"baseline\" small model for our use case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: Uploading the files to cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from google.cloud import storage\n",
    "\n",
    "# Set up your credentials and initialize the client\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'aphasia-chatter-5a70166fc2f1.json'\n",
    "client = storage.Client()\n",
    "\n",
    "# Define the bucket and target directory in the bucket\n",
    "bucket = client.get_bucket('speech-sit-bucket')  # Replace with your bucket name\n",
    "directory_prefix = 'audio'  # Destination prefix in the GCS bucket\n",
    "\n",
    "# Specify your local directory with files to upload\n",
    "local_directory = 'samples/audio'  # Replace with your local directory path\n",
    "\n",
    "# Get list of files in the directory\n",
    "files = [f for f in os.listdir(local_directory) if os.path.isfile(os.path.join(local_directory, f))]\n",
    "\n",
    "# Upload each file with a progress bar\n",
    "with tqdm(total=len(files), desc=\"Uploading files\", unit=\"file\") as pbar:\n",
    "    for filename in files:\n",
    "        local_file_path = os.path.join(local_directory, filename)\n",
    "        blob_path = os.path.join(directory_prefix, filename)\n",
    "        blob = bucket.blob(blob_path)\n",
    "        \n",
    "        # Upload the file and update the progress bar\n",
    "        blob.upload_from_filename(local_file_path)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files from the local directory...\n",
      "Loaded 14159 files from local directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from google.cloud import storage\n",
    "\n",
    "# Set up Google Cloud credentials and initialize the client\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'aphasia-chatter-5a70166fc2f1.json'\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket('speech-sit-bucket')  # Replace with your bucket name\n",
    "\n",
    "# Define GCS directory and local download directory\n",
    "directory_prefix = 'audio/'  # GCS directory prefix\n",
    "download_directory = 'samples/audio'  # Local directory path\n",
    "\n",
    "# Check if the local directory exists\n",
    "if not os.path.exists(download_directory):\n",
    "    # If directory doesn't exist, create it\n",
    "    os.makedirs(download_directory)\n",
    "\n",
    "    # List all blobs (files) in the specified GCS directory\n",
    "    audios = bucket.list_blobs(prefix=directory_prefix)\n",
    "\n",
    "    # Initialize tqdm, but set the total to len(audio_file_set), the actual number of files to download\n",
    "    progress_bar = tqdm(total=14159, desc=\"Downloading Files\", unit=\" files\", leave=False)\n",
    "    for index, audio in enumerate(audios):\n",
    "        if index != 0:\n",
    "            audio_file_name = os.path.basename(audio.name)\n",
    "            local_file_path = os.path.join(download_directory, audio_file_name)\n",
    "            audio.download_to_filename(local_file_path)\n",
    "            progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "else:\n",
    "    # If the directory exists, read files from local storage\n",
    "    print(\"Reading files from the local directory...\")\n",
    "    files = sorted(os.listdir(download_directory))\n",
    "    print(f\"Loaded {len(files)} files from local directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Remove Files > 30s long and Extract Waveforms\n",
    "\n",
    "The purpose of this step is to identify audio files that are longer than 30s. We can then remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files:  88%|████████▊ | 12466/14159 [16:07<01:45, 16.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping hl_e062_A-59.m4a due to error: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files:  98%|█████████▊| 13923/14159 [17:50<00:15, 15.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping hl_e071_A-P-02.m4a due to error: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files: 100%|██████████| 14159/14159 [18:08<00:00, 13.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "input_dir = \"samples/audio\"\n",
    "output_waveform_dir = \"samples/waveform\"\n",
    "output_audio_dir = \"samples/audio_processed\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_audio_dir, exist_ok=True)\n",
    "os.makedirs(output_waveform_dir, exist_ok=True)\n",
    "        \n",
    "def save_audio_and_waveform(file_path, filename, waveform, sample_rate):\n",
    "    # Save the original audio as a single chunk\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Save audio\n",
    "    output_audio_path = os.path.join(output_audio_dir, f\"{base_name}.wav\")\n",
    "    AudioSegment.from_file(file_path).export(output_audio_path, format=\"wav\")\n",
    "    \n",
    "    # Save waveform as .npy file\n",
    "    output_waveform_path = os.path.join(output_waveform_dir, f\"waveform_{base_name}.wav.npy\")\n",
    "    np.save(output_waveform_path, waveform)\n",
    "\n",
    "# Iterate through each audio file in the input directory\n",
    "for filename in tqdm(os.listdir(input_dir), desc=\"Processing audio files\"):\n",
    "    if filename.endswith(\".m4a\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        try:\n",
    "            # Load the audio file with librosa to access waveform and sample rate\n",
    "            waveform, sample_rate = librosa.load(file_path, sr=16000)\n",
    "\n",
    "            # Duration\n",
    "            duration = librosa.get_duration(y=waveform, sr=sample_rate)\n",
    "\n",
    "            # If file > 30s, drop it\n",
    "            if duration <= 30:\n",
    "                save_audio_and_waveform(file_path, filename, waveform, sample_rate)\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Skipping {filename} due to error: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate the \"gold\" transcriptions\n",
    "Use `whisper-large-v3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2.88G/2.88G [00:46<00:00, 65.9MiB/s]\n",
      "Transcribing Audio Files: 100%|██████████| 13769/13769 [2:21:36<00:00,  1.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription complete. Final checkpoint saved.\n",
      "   patient                                      path   audio_base_path  \\\n",
      "0  al_e026  samples/audio_processed/al_e026_A-02.wav  al_e026_A-02.wav   \n",
      "1  al_e026  samples/audio_processed/al_e026_A-03.wav  al_e026_A-03.wav   \n",
      "2  al_e026  samples/audio_processed/al_e026_A-04.wav  al_e026_A-04.wav   \n",
      "3  al_e026  samples/audio_processed/al_e026_A-05.wav  al_e026_A-05.wav   \n",
      "4  al_e026  samples/audio_processed/al_e026_A-06.wav  al_e026_A-06.wav   \n",
      "\n",
      "                                     gold_transcript  \n",
      "0   I do body. And I have a bag. Racking a... ......  \n",
      "1   My body is a frog, frog, frog, cow, cow, cow,...  \n",
      "2   A crack? It looks like a bag of people clicki...  \n",
      "3   The broke, a broke croaking, cracked calf, cr...  \n",
      "4   This is a frog calf. The two persons, Bok is ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch.backends\n",
    "import torch.backends.mps\n",
    "import torch.mps\n",
    "import whisper\n",
    "from whisper.normalizers import BasicTextNormalizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"large-v3\")\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Specify directory with audio files and checkpoint file path\n",
    "audio_directory = 'samples/audio_processed'\n",
    "checkpoint_path = 'transcription_checkpoint.csv'\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    df = pd.read_csv(checkpoint_path)\n",
    "    processed_files = set(df['audio_base_path'])\n",
    "    print(f\"Resuming from checkpoint. {len(processed_files)} files already processed.\")\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\"patient\", \"path\", \"audio_base_path\", \"gold_transcript\"])\n",
    "    processed_files = set()\n",
    "\n",
    "# List audio files in the directory\n",
    "audio_files = [f for f in os.listdir(audio_directory) if os.path.isfile(os.path.join(audio_directory, f))]\n",
    "\n",
    "# Transcribe each audio file and store results in DataFrame format\n",
    "for audio_file in tqdm(audio_files, desc=\"Transcribing Audio Files\"):\n",
    "    if audio_file in processed_files:\n",
    "        continue  # Skip already processed files\n",
    "\n",
    "    audio_path = os.path.join(audio_directory, audio_file)\n",
    "    audio_base_path = os.path.basename(audio_path)\n",
    "\n",
    "    try:\n",
    "        # Transcribe the audio\n",
    "        result = model.transcribe(audio_path, language=\"en\")\n",
    "        gold_transcript = result[\"text\"]\n",
    "\n",
    "        # Ensure that patient file name is in the format 'al_xxxx' or 'hl_xxxx'\n",
    "        patient = audio_base_path[:7]\n",
    "\n",
    "        # Create a new row to add\n",
    "        new_row = pd.DataFrame({\n",
    "            \"patient\": [patient],\n",
    "            \"path\": [audio_path],\n",
    "            \"audio_base_path\": [audio_base_path],\n",
    "            \"gold_transcript\": [gold_transcript]\n",
    "        })\n",
    "\n",
    "        # Append the new row to the DataFrame\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "        # Save checkpoint every 10 files\n",
    "        if len(df) % 10 == 0:\n",
    "            df.to_csv(checkpoint_path, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error and continue with the next file\n",
    "        print(f\"Error processing {audio_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Final save to CSV after processing all files\n",
    "df.to_csv(checkpoint_path, index=False)\n",
    "print(\"Transcription complete. Final checkpoint saved.\")\n",
    "\n",
    "# Display the DataFrame to verify\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13680 entries, 0 to 13679\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   patient          13680 non-null  object\n",
      " 1   path             13680 non-null  object\n",
      " 2   audio_base_path  13680 non-null  object\n",
      " 3   gold_transcript  13680 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 427.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(ignore_index=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>path</th>\n",
       "      <th>audio_base_path</th>\n",
       "      <th>gold_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13675</th>\n",
       "      <td>hl_e072</td>\n",
       "      <td>samples/audio_processed/hl_e072_B-48.wav</td>\n",
       "      <td>hl_e072_B-48.wav</td>\n",
       "      <td>Running.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13676</th>\n",
       "      <td>hl_e072</td>\n",
       "      <td>samples/audio_processed/hl_e072_B-49.wav</td>\n",
       "      <td>hl_e072_B-49.wav</td>\n",
       "      <td>Sewing with a needle?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13677</th>\n",
       "      <td>hl_e072</td>\n",
       "      <td>samples/audio_processed/hl_e072_B-50.wav</td>\n",
       "      <td>hl_e072_B-50.wav</td>\n",
       "      <td>Shaving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13678</th>\n",
       "      <td>hl_e072</td>\n",
       "      <td>samples/audio_processed/hl_e072_B-51.wav</td>\n",
       "      <td>hl_e072_B-51.wav</td>\n",
       "      <td>Shooting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13679</th>\n",
       "      <td>hl_e072</td>\n",
       "      <td>samples/audio_processed/hl_e072_B-52.wav</td>\n",
       "      <td>hl_e072_B-52.wav</td>\n",
       "      <td>Singing.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       patient                                      path   audio_base_path  \\\n",
       "13675  hl_e072  samples/audio_processed/hl_e072_B-48.wav  hl_e072_B-48.wav   \n",
       "13676  hl_e072  samples/audio_processed/hl_e072_B-49.wav  hl_e072_B-49.wav   \n",
       "13677  hl_e072  samples/audio_processed/hl_e072_B-50.wav  hl_e072_B-50.wav   \n",
       "13678  hl_e072  samples/audio_processed/hl_e072_B-51.wav  hl_e072_B-51.wav   \n",
       "13679  hl_e072  samples/audio_processed/hl_e072_B-52.wav  hl_e072_B-52.wav   \n",
       "\n",
       "              gold_transcript  \n",
       "13675                Running.  \n",
       "13676   Sewing with a needle?  \n",
       "13677                 Shaving  \n",
       "13678                Shooting  \n",
       "13679                Singing.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "patients = df['patient'].unique()\n",
    "np.random.shuffle(patients)\n",
    "\n",
    "train_patients = patients[:int(0.7 * len(patients))]\n",
    "val_patients = patients[int(0.7 * len(patients)):int(0.85 * len(patients))]\n",
    "test_patients = patients[int(0.85 * len(patients)):]\n",
    "\n",
    "train_df = df[df['patient'].isin(train_patients)]\n",
    "val_df = df[df['patient'].isin(val_patients)]\n",
    "test_df = df[df['patient'].isin(test_patients)]\n",
    "\n",
    "train_df.to_csv('train_set.csv', index=False)\n",
    "val_df.to_csv('val_set.csv', index=False)\n",
    "test_df.to_csv('test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Export the transcripts to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded train_set.csv to transcripts/train_set.csv\n",
      "Uploaded val_set.csv to transcripts/val_set.csv\n",
      "Uploaded test_set.csv to transcripts/test_set.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from google.cloud import storage\n",
    "\n",
    "# Set up your credentials and initialize the client\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'aphasia-chatter-5a70166fc2f1.json'\n",
    "client = storage.Client()\n",
    "\n",
    "# Define the bucket and target directory in the bucket\n",
    "bucket = client.get_bucket('speech-sit-bucket')  # Replace with your bucket name\n",
    "\n",
    "directory_prefix = 'transcripts/'\n",
    "\n",
    "# Function to upload a file to the specified GCS bucket path\n",
    "def upload_to_bucket(local_file, bucket_path):\n",
    "    blob = bucket.blob(bucket_path)\n",
    "    blob.upload_from_filename(local_file)\n",
    "    print(f\"Uploaded {local_file} to {bucket_path}\")\n",
    "\n",
    "# Upload each CSV file to the bucket\n",
    "upload_to_bucket('train_set.csv', os.path.join(directory_prefix, 'train_set.csv'))\n",
    "upload_to_bucket('val_set.csv', os.path.join(directory_prefix, 'val_set.csv'))\n",
    "upload_to_bucket('test_set.csv', os.path.join(directory_prefix, 'test_set.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Export the waveforms to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files: 100%|██████████| 13769/13769 [2:47:44<00:00,  1.37file/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from google.cloud import storage\n",
    "import time\n",
    "\n",
    "# Set up your credentials and initialize the client\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'aphasia-chatter-5a70166fc2f1.json'\n",
    "client = storage.Client()\n",
    "\n",
    "# Define the bucket and target directory in the bucket\n",
    "bucket = client.get_bucket('speech-sit-bucket')  # Replace with your bucket name\n",
    "directory_prefix = 'waveform'  # Destination prefix in the GCS bucket\n",
    "\n",
    "# Specify your local directory with files to upload\n",
    "local_directory = 'samples/waveform'  # Replace with your local directory path\n",
    "\n",
    "# Get list of files in the directory\n",
    "files = [f for f in os.listdir(local_directory) if os.path.isfile(os.path.join(local_directory, f))]\n",
    "\n",
    "# Upload each file with a progress bar\n",
    "with tqdm(total=len(files), desc=\"Uploading files\", unit=\"file\") as pbar:\n",
    "    for filename in files:\n",
    "        local_file_path = os.path.join(local_directory, filename)\n",
    "        blob_path = os.path.join(directory_prefix, filename)\n",
    "        blob = bucket.blob(blob_path)\n",
    "        \n",
    "        # Upload the file and update the progress bar\n",
    "        blob.upload_from_filename(local_file_path)\n",
    "        pbar.update(1)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
